{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Functions and Optimization in AI\n",
        "\n",
        "Welcome! This beginner-friendly notebook takes you from intuition to math to code for loss functions and optimization. You'll read short stories and analogies, learn the formulas with every symbol explained, and then see visualizations and simulations.\n",
        "\n",
        "What you will learn:\n",
        "- Why we need loss functions and optimization\n",
        "- The most common losses: MSE, MAE, Cross-Entropy, Hinge, KL-Divergence, plus Huber and Focal\n",
        "- Optimizers: Gradient Descent, SGD, Mini-batch, Momentum, Nesterov, AdaGrad, RMSProp, Adam\n",
        "- How learning rate, batch size, and epochs affect training\n",
        "- Visual intuition: loss surfaces and optimization paths\n",
        "- Real-world examples and step-by-step gradient updates\n",
        "- Practice problems and experiments you can try\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: imports and plotting defaults\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
        "\n",
        "# Utility: helper to annotate plots\n",
        "def despine(axis=None):\n",
        "    ax = axis or plt.gca()\n",
        "    sns.despine(ax=ax)\n",
        "\n",
        "# Utility: meshgrid for contour plots\n",
        "\n",
        "def make_mesh(xmin, xmax, ymin, ymax, steps=200):\n",
        "    x = np.linspace(xmin, xmax, steps)\n",
        "    y = np.linspace(ymin, ymax, steps)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    return X, Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why do we need loss functions? (Story)\n",
        "Imagine teaching a robot to throw a ball into a basket. After each throw, you need a way to tell it \"how bad\" the throw was. Was it 2 meters short? 10 cm to the left? That number—the \"badness\" of the attempt—is the loss. The robot uses this number to learn to throw better next time.\n",
        "\n",
        "- Without a loss, the robot gets no feedback. No feedback means no learning.\n",
        "- The loss converts \"performance\" into a single number we can minimize.\n",
        "- Smaller loss → better predictions.\n",
        "\n",
        "In machine learning, we predict something (price, class, probability), compare it to the true answer, compute the loss, and adjust parameters to make future predictions better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intuition: Real-world analogies\n",
        "- **GPS navigation**: The loss is like your distance to the destination. Optimization is choosing turns to get closer each minute.\n",
        "- **Darts**: Each dart lands somewhere. The loss is the distance from the bullseye. The strategy you use to correct your aim is optimization.\n",
        "- **Cooking to taste**: Taste (loss) tells you how far your soup is from \"just right.\" Adjusting salt/heat (optimization) moves you toward better taste.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why do we need optimization? (Story + Analogy)\n",
        "You have a map of hills and valleys (the loss surface). Your model's parameters are your location on this map. You want to get to the lowest valley (minimum loss). Optimization is your hiking strategy:\n",
        "- Look at the slope under your feet (gradient)\n",
        "- Step downhill (update)\n",
        "- Keep stepping until you reach a low place\n",
        "\n",
        "Different strategies (optimizers) choose step sizes and directions differently to reach the valley faster and more reliably.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Functions: Definitions, intuition, and when to use\n",
        "\n",
        "#### Mean Squared Error (MSE)\n",
        "Formula: \\( \\text{MSE}(\\hat{y}, y) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 \\)\n",
        "- \\(\\hat{y}_i\\): predicted value for example i\n",
        "- \\(y_i\\): true value for example i\n",
        "- \\(n\\): number of examples\n",
        "- Squaring heavily penalizes large errors → sensitive to outliers\n",
        "- **Use when**: regression with Gaussian-like noise; smooth, convex, differentiable\n",
        "\n",
        "#### Mean Absolute Error (MAE)\n",
        "Formula: \\( \\text{MAE}(\\hat{y}, y) = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}_i - y_i| \\)\n",
        "- Absolute value penalizes linearly → robust to outliers\n",
        "- Non-differentiable at 0, but subgradients exist\n",
        "- **Use when**: you want median-like behavior, robustness to outliers\n",
        "\n",
        "#### Binary Cross-Entropy (Log Loss)\n",
        "Formula: \\( \\text{BCE}(\\hat{p}, y) = -\\frac{1}{n}\\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i) \\right] \\)\n",
        "- \\(\\hat{p}_i\\): predicted probability of class 1\n",
        "- \\(y_i \\in \\{0,1\\}\\): true label\n",
        "- Derived from maximum likelihood under Bernoulli model\n",
        "- **Use when**: binary classification\n",
        "\n",
        "#### Multiclass Cross-Entropy\n",
        "Softmax: \\( \\hat{p}_{i,k} = \\frac{e^{z_{i,k}}}{\\sum_j e^{z_{i,j}}} \\), Loss: \\( -\\frac{1}{n} \\sum_{i=1}^n \\sum_{k} y_{i,k}\\log(\\hat{p}_{i,k}) \\)\n",
        "- \\(z_{i,k}\\): logit for class k; \\(y_{i,k}\\): one-hot label\n",
        "- **Use when**: multiclass classification\n",
        "\n",
        "#### Hinge Loss (for SVM)\n",
        "Binary (labels \\(y\\in\\{-1, +1\\}\\)): \\( \\max(0, 1 - y\\cdot f(x)) \\)\n",
        "- Encourages a margin: predictions not just correct, but confidently correct\n",
        "- **Use when**: margin-based classifiers (SVM)\n",
        "\n",
        "#### KL-Divergence (information distance)\n",
        "\\( D_{\\mathrm{KL}}(P\\,\\Vert\\,Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\)\n",
        "- Measures how one probability distribution \\(Q\\) diverges from true distribution \\(P\\)\n",
        "- Asymmetric; not a metric\n",
        "- **Use when**: measuring distribution mismatch (e.g., VAEs, distillation)\n",
        "\n",
        "### Properties and connections\n",
        "- **Convexity**: MSE, MAE, hinge are convex; cross-entropy is convex for logistic regression; KL is convex in Q\n",
        "- **Bias-variance link**: MSE decomposes into bias^2 + variance + noise\n",
        "- **Likelihood**: MSE ↔ Gaussian, MAE ↔ Laplace, BCE ↔ Bernoulli, CE ↔ categorical\n",
        "- **Choosing a loss**:\n",
        "  - Noisy Gaussian-ish regression → MSE\n",
        "  - Robust regression → MAE or Huber\n",
        "  - Class probabilities → Cross-Entropy\n",
        "  - Margins and robustness to outliers → Hinge\n",
        "  - Distribution matching → KL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom/Composite Losses\n",
        "\n",
        "#### Huber Loss (smooth L1)\n",
        "\\[ \\mathcal{L}_\\delta(r) = \\begin{cases}\n",
        " \\tfrac{1}{2} r^2 & \\text{if } |r| \\le \\delta \\\\\n",
        " \\delta(|r| - \\tfrac{1}{2}\\delta) & \\text{otherwise}\n",
        "\\end{cases} \\]\n",
        "- \\(r = \\hat{y} - y\\) is the residual; \\(\\delta\\) is a tuning threshold\n",
        "- Quadratic near 0 (like MSE), linear in tails (like MAE)\n",
        "- **Use when**: you want MSE smoothness but MAE robustness\n",
        "\n",
        "#### Focal Loss (for class imbalance)\n",
        "Binary: \\( \\mathrm{FL}(\\hat{p}, y) = -\\alpha (1-\\hat{p})^{\\gamma} y\\log(\\hat{p}) - (1-\\alpha) \\hat{p}^{\\gamma}(1-y)\\log(1-\\hat{p}) \\)\n",
        "- \\(\\gamma\\): focusing parameter, down-weights easy examples\n",
        "- \\(\\alpha\\): class-balancing factor\n",
        "- **Use when**: heavy class imbalance (e.g., fraud detection, detection tasks)\n",
        "\n",
        "Notes:\n",
        "- Composite losses often add regularization terms: \\(\\mathcal{L}_\\text{total} = \\mathcal{L}_\\text{data} + \\lambda \\|\\theta\\|^2\\)\n",
        "- Scaling and normalization matter for training stability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization Algorithms: from gradients to updates\n",
        "\n",
        "### Gradient Descent (batch)\n",
        "- Update: \\( \\theta_{t+1} = \\theta_t - \\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta_t) \\)\n",
        "- \\(\\eta\\): learning rate; \\(\\nabla_\\theta \\mathcal{L}\\): gradient over full dataset\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)\n",
        "- Use one sample at a time: \\( \\theta_{t+1} = \\theta_t - \\eta \\, \\nabla_\\theta \\ell(\\theta_t; x_i, y_i) \\)\n",
        "- Noisy but fast; enables online learning\n",
        "\n",
        "### Mini-batch SGD\n",
        "- Use small batches: balances stability and speed\n",
        "\n",
        "### Practical notes\n",
        "- Too large \\(\\eta\\) → divergence; too small → slow\n",
        "- Shuffle data every epoch for SGD/mini-batch\n",
        "- Normalize inputs to improve conditioning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Momentum, Nesterov, and Adaptive Methods\n",
        "\n",
        "#### Momentum\n",
        "\\[ v_{t+1} = \\beta v_t + (1-\\beta)\\, \\nabla_\\theta \\mathcal{L}(\\theta_t), \\quad \\theta_{t+1} = \\theta_t - \\eta \\, v_{t+1} \\]\n",
        "- \\(\\beta\\): momentum coefficient (e.g., 0.9). Averages gradients to smooth zig-zag.\n",
        "\n",
        "#### Nesterov Accelerated Gradient (NAG)\n",
        "Look ahead by momentum before computing gradient:\n",
        "\\[ v_{t+1} = \\beta v_t + (1-\\beta)\\, \\nabla_\\theta \\mathcal{L}(\\theta_t - \\eta \\beta v_t) \\]\n",
        "\\[ \\theta_{t+1} = \\theta_t - \\eta v_{t+1} \\]\n",
        "\n",
        "#### AdaGrad\n",
        "\\[ G_{t+1} = G_t + g_t \\odot g_t, \\quad \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_{t+1}} + \\epsilon} \\odot g_t \\]\n",
        "- Per-parameter learning rates shrink over time; good for sparse features\n",
        "\n",
        "#### RMSProp\n",
        "Exponentially decaying average of squared gradients:\n",
        "\\[ s_{t+1} = \\rho s_t + (1-\\rho) \\, g_t^2, \\quad \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_{t+1}} + \\epsilon} g_t \\]\n",
        "\n",
        "#### Adam\n",
        "Combines momentum (first moment) + RMSProp (second moment):\n",
        "\\[ m_{t+1} = \\beta_1 m_t + (1-\\beta_1) g_t, \\quad v_{t+1} = \\beta_2 v_t + (1-\\beta_2) g_t^2 \\]\n",
        "Bias-corrected:\n",
        "\\[ \\hat{m}_{t+1} = \\frac{m_{t+1}}{1-\\beta_1^{t+1}}, \\; \\hat{v}_{t+1} = \\frac{v_{t+1}}{1-\\beta_2^{t+1}}, \\; \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon} \\]\n",
        "\n",
        "Notes:\n",
        "- Adam often works out-of-the-box (\\(\\eta\\approx 1e-3\\))\n",
        "- Tune \\(\\beta_1, \\beta_2\\) for stability (common: 0.9, 0.999)\n",
        "- Consider decoupled weight decay for regularization (AdamW)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Dynamics: LR, batch size, epochs, and pitfalls\n",
        "- **Learning rate (\\(\\eta\\))**: too high → oscillation/divergence; too low → slow\n",
        "- **Batch size**: small → noisy but explores valleys; large → stable but may get stuck\n",
        "- **Epochs**: number of full passes; watch for overfitting\n",
        "- **Challenges**: local minima (rare in deep nets), saddle points, plateaus, poor conditioning (zig-zag), exploding/vanishing gradients\n",
        "- **Fixes**: normalization, good initialization, momentum/Adam, LR schedules, gradient clipping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic dataset for regression (for MSE surface)\n",
        "# y = 2.0 * x + 1.0 + noise\n",
        "rng = np.random.default_rng(0)\n",
        "x_reg = rng.uniform(-3, 3, size=60)\n",
        "noise = rng.normal(0, 0.6, size=x_reg.shape)\n",
        "y_reg = 2.0 * x_reg + 1.0 + noise\n",
        "\n",
        "# Grid over parameters (w, b)\n",
        "w_grid = np.linspace(-1.5, 4.0, 120)\n",
        "b_grid = np.linspace(-2.0, 4.0, 120)\n",
        "W, B = np.meshgrid(w_grid, b_grid)\n",
        "\n",
        "# Compute MSE surface\n",
        "Y_hat = W[None, :, :] * x_reg[:, None, None] + B[None, :, :]\n",
        "residuals = Y_hat - y_reg[:, None, None]\n",
        "MSE_surface = np.mean(residuals**2, axis=0)\n",
        "\n",
        "# Plot contours\n",
        "plt.figure(figsize=(7, 5))\n",
        "contours = plt.contour(W, B, MSE_surface, levels=30, cmap=\"viridis\")\n",
        "plt.clabel(contours, inline=True, fontsize=8, fmt=\"%.2f\")\n",
        "plt.title(\"MSE Loss Surface over (w, b)\")\n",
        "plt.xlabel(\"w\")\n",
        "plt.ylabel(\"b\")\n",
        "despine()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary logistic regression toy example for Cross-Entropy visualization\n",
        "rng = np.random.default_rng(1)\n",
        "N = 100\n",
        "x_pos = rng.normal(1.5, 0.6, size=(N//2, 2))\n",
        "x_neg = rng.normal(-1.5, 0.6, size=(N//2, 2))\n",
        "X_cls = np.vstack([x_pos, x_neg])\n",
        "y_cls = np.hstack([np.ones(N//2), np.zeros(N//2)])\n",
        "\n",
        "# Logistic regression model: p = sigmoid(w^T x + b)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Fix b and visualize CE over a 2D grid of w = (w1, w2)\n",
        "w1 = np.linspace(-4, 4, 121)\n",
        "w2 = np.linspace(-4, 4, 121)\n",
        "W1, W2 = np.meshgrid(w1, w2)\n",
        "bias = 0.0\n",
        "Z = W1[None, :, :] * X_cls[:, 0][:, None, None] + W2[None, :, :] * X_cls[:, 1][:, None, None] + bias\n",
        "P = sigmoid(Z)\n",
        "# Binary cross-entropy for each point, averaged across samples\n",
        "CE_surface = -np.mean(y_cls[:, None, None] * np.log(P + 1e-10) + (1 - y_cls)[:, None, None] * np.log(1 - P + 1e-10), axis=0)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "contours = plt.contour(W1, W2, CE_surface, levels=30, cmap=\"magma\")\n",
        "plt.clabel(contours, inline=True, fontsize=8, fmt=\"%.2f\")\n",
        "plt.title(\"Cross-Entropy Loss over (w1, w2) with b=0\")\n",
        "plt.xlabel(\"w1\")\n",
        "plt.ylabel(\"w2\")\n",
        "despine()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer simulations on a 2D quadratic bowl to visualize paths\n",
        "# Loss: f(w) = 0.5 * [a*w1^2 + b*w2^2] (ill-conditioned if a != b)\n",
        "a, b = 1.0, 10.0\n",
        "\n",
        "def f(w):\n",
        "    return 0.5 * (a * w[0]**2 + b * w[1]**2)\n",
        "\n",
        "def grad_f(w):\n",
        "    return np.array([a * w[0], b * w[1]])\n",
        "\n",
        "w0 = np.array([3.5, 3.5])\n",
        "T = 60\n",
        "\n",
        "# Trajectories for different optimizers\n",
        "\n",
        "def run_gd(eta):\n",
        "    w = w0.copy()\n",
        "    traj = [w.copy()]\n",
        "    for t in range(T):\n",
        "        g = grad_f(w)\n",
        "        w = w - eta * g\n",
        "        traj.append(w.copy())\n",
        "    return np.array(traj)\n",
        "\n",
        "\n",
        "def run_momentum(eta, beta=0.9):\n",
        "    w = w0.copy()\n",
        "    v = np.zeros_like(w)\n",
        "    traj = [w.copy()]\n",
        "    for t in range(T):\n",
        "        g = grad_f(w)\n",
        "        v = beta * v + (1 - beta) * g\n",
        "        w = w - eta * v\n",
        "        traj.append(w.copy())\n",
        "    return np.array(traj)\n",
        "\n",
        "\n",
        "def run_adam(eta=0.1, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "    w = w0.copy()\n",
        "    m = np.zeros_like(w)\n",
        "    v = np.zeros_like(w)\n",
        "    traj = [w.copy()]\n",
        "    for t in range(1, T + 1):\n",
        "        g = grad_f(w)\n",
        "        m = beta1 * m + (1 - beta1) * g\n",
        "        v = beta2 * v + (1 - beta2) * (g * g)\n",
        "        m_hat = m / (1 - beta1**t)\n",
        "        v_hat = v / (1 - beta2**t)\n",
        "        w = w - eta * m_hat / (np.sqrt(v_hat) + eps)\n",
        "        traj.append(w.copy())\n",
        "    return np.array(traj)\n",
        "\n",
        "traj_gd = run_gd(eta=0.15)\n",
        "traj_mom = run_momentum(eta=0.3, beta=0.9)\n",
        "traj_adam = run_adam(eta=0.25)\n",
        "\n",
        "# Plot the bowl and the paths\n",
        "X, Y = make_mesh(-4, 4, -4, 4, steps=200)\n",
        "Z = 0.5 * (a * X**2 + b * Y**2)\n",
        "plt.figure(figsize=(7, 6))\n",
        "cs = plt.contour(X, Y, Z, levels=30, cmap=\"Greys\")\n",
        "plt.clabel(cs, inline=True, fontsize=8, fmt=\"%.1f\")\n",
        "plt.plot(traj_gd[:, 0], traj_gd[:, 1], \"-o\", ms=3, label=\"GD (eta=0.15)\")\n",
        "plt.plot(traj_mom[:, 0], traj_mom[:, 1], \"-o\", ms=3, label=\"Momentum (eta=0.3)\")\n",
        "plt.plot(traj_adam[:, 0], traj_adam[:, 1], \"-o\", ms=3, label=\"Adam (eta=0.25)\")\n",
        "plt.legend()\n",
        "plt.title(\"Optimization Paths on an Ill-Conditioned Quadratic\")\n",
        "plt.xlabel(\"w1\")\n",
        "plt.ylabel(\"w2\")\n",
        "despine()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SGD vs batch GD on linear regression with adjustable batch size and LR\n",
        "rng = np.random.default_rng(2)\n",
        "x = rng.uniform(-2, 2, size=200)\n",
        "y = 1.2 * x - 0.7 + rng.normal(0, 0.3, size=x.shape)\n",
        "X = np.c_[x, np.ones_like(x)]  # [x, 1]\n",
        "\n",
        "true_w = np.array([1.2, -0.7])\n",
        "\n",
        "\n",
        "def mse(yhat, y):\n",
        "    return np.mean((yhat - y) ** 2)\n",
        "\n",
        "\n",
        "def grad_mse_wrt_w(Xb, yb, w):\n",
        "    # gradient of MSE w.r.t. w for batch Xb\n",
        "    yhat = Xb @ w\n",
        "    return (2.0 / len(Xb)) * (Xb.T @ (yhat - yb))\n",
        "\n",
        "\n",
        "def train_sgd(X, y, lr=0.1, batch_size=1, epochs=20, method=\"sgd\"):\n",
        "    w = rng.normal(0, 1, size=2)\n",
        "    hist = {\"w\": [w.copy()], \"loss\": []}\n",
        "    for ep in range(epochs):\n",
        "        idx = rng.permutation(len(X))\n",
        "        Xs, ys = X[idx], y[idx]\n",
        "        for i in range(0, len(Xs), batch_size):\n",
        "            Xb = Xs[i : i + batch_size]\n",
        "            yb = ys[i : i + batch_size]\n",
        "            g = grad_mse_wrt_w(Xb, yb, w)\n",
        "            w = w - lr * g\n",
        "            hist[\"w\"].append(w.copy())\n",
        "            hist[\"loss\"].append(mse(X @ w, y))\n",
        "    hist[\"w\"] = np.array(hist[\"w\"])  # (steps, 2)\n",
        "    hist[\"loss\"] = np.array(hist[\"loss\"])  # (steps,)\n",
        "    return w, hist\n",
        "\n",
        "# Run experiments\n",
        "w_sgd, h_sgd = train_sgd(X, y, lr=0.2, batch_size=1, epochs=10)\n",
        "w_mb, h_mb = train_sgd(X, y, lr=0.1, batch_size=32, epochs=10)\n",
        "w_bg, h_bg = train_sgd(X, y, lr=0.05, batch_size=len(X), epochs=10)\n",
        "\n",
        "# Plot loss curves\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(h_sgd[\"loss\"], label=\"SGD (bs=1, lr=0.2)\")\n",
        "plt.plot(h_mb[\"loss\"], label=\"Mini-batch (bs=32, lr=0.1)\")\n",
        "plt.plot(h_bg[\"loss\"], label=\"Batch GD (full, lr=0.05)\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"MSE (log scale)\")\n",
        "plt.title(\"Effect of Batch Size and Learning Rate on Convergence\")\n",
        "plt.legend()\n",
        "despine()\n",
        "plt.show()\n",
        "\n",
        "# Plot parameter paths in (w0, w1)\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(h_sgd[\"w\"][:, 0], h_sgd[\"w\"][:, 1], label=\"SGD\")\n",
        "plt.plot(h_mb[\"w\"][:, 0], h_mb[\"w\"][:, 1], label=\"Mini-batch\")\n",
        "plt.plot(h_bg[\"w\"][:, 0], h_bg[\"w\"][:, 1], label=\"Batch GD\")\n",
        "plt.scatter([true_w[0]], [true_w[1]], c=\"red\", marker=\"*\", s=120, label=\"True\")\n",
        "plt.xlabel(\"w_slope\")\n",
        "plt.ylabel(\"w_bias\")\n",
        "plt.title(\"Parameter Trajectories\")\n",
        "plt.legend()\n",
        "despine()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-world scenarios: Choosing losses and optimizers\n",
        "\n",
        "1. **Linear regression for housing prices**\n",
        "   - Loss: MSE (noise roughly Gaussian, continuous target)\n",
        "   - Optimizer: Mini-batch SGD or Adam\n",
        "   - Tip: Standardize features; start with LR ~ 1e-2 to 1e-3 for Adam\n",
        "\n",
        "2. **Fraud detection (high class imbalance)**\n",
        "   - Loss: Binary Cross-Entropy + Focal Loss variant or class weights\n",
        "   - Optimizer: Adam (handles sparse informative features well)\n",
        "   - Tip: Monitor precision/recall; use AUROC/PR AUC; consider undersampling/oversampling\n",
        "\n",
        "3. **Image classification (multiclass)**\n",
        "   - Loss: Cross-Entropy with softmax\n",
        "   - Optimizer: SGD with Momentum or Adam; cosine LR schedule\n",
        "   - Tip: Use data augmentation; weight decay to regularize\n",
        "\n",
        "4. **SVM-style margin classification**\n",
        "   - Loss: Hinge\n",
        "   - Optimizer: specialized solvers or SGD on hinge loss\n",
        "   - Tip: Scaling matters; hinge encourages larger margins\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worked examples: step-by-step gradients and updates\n",
        "\n",
        "### Example A: Linear regression (one step)\n",
        "Model: \\(\\hat{y} = wx + b\\), Loss: \\(\\ell = (\\hat{y} - y)^2\\)\n",
        "- Given: \\(x=2\\), \\(y=5\\), current \\(w=1\\), \\(b=0\\), \\(\\eta=0.1\\)\n",
        "- Forward: \\(\\hat{y}=1\\cdot 2 + 0 = 2\\)\n",
        "- Residual: \\(r = \\hat{y}-y = -3\\)\n",
        "- Gradients: \\(\\partial \\ell/\\partial w = 2 r x = -12\\), \\(\\partial \\ell/\\partial b = 2 r = -6\\)\n",
        "- Update: \\(w' = 1 - 0.1(-12) = 2.2\\), \\(b' = 0 - 0.1(-6) = 0.6\\)\n",
        "\n",
        "### Example B: Logistic regression (binary) with BCE\n",
        "Model: \\(p = \\sigma(wx+b)\\), Loss: \\(\\ell = -[y\\log p + (1-y)\\log(1-p)]\\)\n",
        "- Given: \\(x=1.0\\), \\(y=1\\), current \\(w=0\\), \\(b=0\\), \\(\\eta=0.5\\)\n",
        "- Forward: \\(z=0\\Rightarrow p=0.5\\)\n",
        "- Gradient: \\(\\partial \\ell/\\partial w = (p-y)x = -0.5\\), \\(\\partial \\ell/\\partial b = (p-y) = -0.5\\)\n",
        "- Update: \\(w' = 0 - 0.5(-0.5) = 0.25\\), \\(b' = 0 - 0.5(-0.5) = 0.25\\)\n",
        "\n",
        "### Example C: Huber loss derivative\n",
        "- Residual \\(r=\\hat{y}-y\\)\n",
        "- If \\(|r| \\le \\delta\\): gradient like MSE, \\(\\partial \\ell/\\partial \\hat{y} = r\\)\n",
        "- Else: like MAE, \\(\\partial \\ell/\\partial \\hat{y} = \\delta\\,\\mathrm{sign}(r)\\)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice problems\n",
        "\n",
        "1. Compute one GD step for linear regression\n",
        "   - Data: \\((x, y) = (3, 10)\\)\n",
        "   - Current: \\(w=2, b=1\\), \\(\\eta=0.05\\)\n",
        "   - Loss: MSE on this single sample. What are \\(w'\\) and \\(b'\\)?\n",
        "\n",
        "2. Logistic regression gradient by hand\n",
        "   - \\(x=2.0\\), \\(y=0\\), \\(w=1.0\\), \\(b=-1.0\\), \\(\\eta=0.2\\)\n",
        "   - Compute \\(p=\\sigma(wx+b)\\), gradients, and updated \\(w', b'\\)\n",
        "\n",
        "3. Implement Huber loss\n",
        "   - Modify the code to implement Huber and compare with MSE for outliers.\n",
        "\n",
        "4. Experiment: learning rate and batch size\n",
        "   - Change `lr` and `batch_size` in the SGD cell. Observe loss curves.\n",
        "\n",
        "5. Optimizer swap\n",
        "   - Reuse the quadratic bowl cell and add RMSProp. Compare its path to Adam.\n",
        "\n",
        "6. Class imbalance exercise\n",
        "   - Modify the BCE surface cell to add class weights or focal term. Observe differences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary / Key Takeaways\n",
        "- **Loss is feedback**: it turns prediction quality into a single number to minimize.\n",
        "- **Pick the right loss**: MSE/MAE for regression, Cross-Entropy for probabilities, Hinge for margins, KL for distributions.\n",
        "- **Optimization is the route down the hill**: GD, SGD, Momentum, Adam each balance speed and stability.\n",
        "- **Hyperparameters matter**: learning rate, batch size, epochs can radically change training.\n",
        "- **Practical heuristics**: normalize inputs, start with Adam (1e-3), later try SGD+Momentum for final polish.\n",
        "\n",
        "### Cheat-sheet\n",
        "| Loss Function | Best Use Case | Optimizer | Why |\n",
        "|---|---|---|---|\n",
        "| MSE | Regression with Gaussian noise | Mini-batch SGD/Adam | Smooth, convex; well-understood |\n",
        "| MAE | Robust regression (outliers) | Adam | Linear penalty; robust to outliers |\n",
        "| BCE | Binary classification | Adam | Probabilistic; calibrated outputs |\n",
        "| CE (softmax) | Multiclass classification | SGD+Momentum/Adam | Stable training, good generalization |\n",
        "| Hinge | Margin-based classification | SGD | Encourages large margins |\n",
        "| KL Divergence | Distribution matching | Adam | Works with probabilistic models |\n",
        "| Huber | Mix of MSE/MAE | Adam | Smooth near 0, robust in tails |\n",
        "| Focal | Class imbalance | Adam | Focuses on hard examples |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Modify and explore\n",
        "- Try different initial points in the optimizer path cell. Do you still see zig-zag with GD?\n",
        "- Change the conditioning of the bowl (set `a=1, b=100`). How do optimizers behave?\n",
        "- In the logistic CE surface, move `b` away from 0 and observe changes.\n",
        "- Add Nesterov or RMSProp implementations and compare paths visually.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
